---
show: step
version: 1.0
enable_checker: true
---

# 并发服务编程实现

## 实验介绍

本实验主要介绍什么是并发服务以及代码实现。

#### 知识点

- 进程、线程和协程的介绍
- 多进程并发服务
- 多线程并发服务
- 多协程并发服务
- 多路 I/O 转接服务

## 进程、线程和协程的介绍

#### 什么是进程

众所周知，计算机的核心是 CPU，它主要负责程序的所有计算任务；而进程则是一个具有一定独立功能的程序，也可以理解为在一个数据集上的一次动态执行的过程，是操作系统进行资源分配和调度的一个独立单位，是一种抽象的概念，一般由程序、数据集合和程序控制块三部分组成。

- 程序一般用于描述进程要完成的功能，是控制进程执行的指令集；
- 数据集合是程序在执行时所需要的数据和工作空间；
- 程序控制块（简称 PCB），它包含进程的描述信息和控制信息，唯一标识进程的存在。

#### 什么是线程

线程也是操作系统提供的一种抽象概念，是程序执行中一个比较单一的顺序控制流程，即程序执行的最小单元，可以理解为处理器调度的基本单位。一个进程可以有一个或多个线程，同一进程中的多个线程是共享该进程中的全部系统资源的，比如虚拟地址空间、文件描述符和信号处理等资源，但同一进程中的多个线程有各自的调用栈信息以及存储本地线程空间。

同样，操作系统也会为线程分配一个线程控制块（简称 TCB），将所有用于控制和管理线程的信息记录在线程的控制块中，线程控制块通常包括以下几个部分：

- 线程标志符：作为线程的唯一标识
- 一组寄存器：用于存储线程信息
- 线程运行状态：线程当前运行的状态
- 优先级：线程运行的优先级

我们来看下线程具有这几种状态：初始态、就绪状态、等待（阻塞）状态、执行状态和终止状态，线程的生命周期如下图所示：

![1](./images/5-1.jpeg)

#### 什么是协程

协程又称为微线程，顾名思义协程是一种比线程更加轻量级的执行过程，不是被操作系统内核所管理，而完全是由程序所控制，即基于用户态的。

可以理解为一个线程中有多个子程序，但执行过程中，子程序内部可中断，然后转而执行别的子程序，在执行完别的子程序后再返回来继续执行刚刚中断的子程序。协程之间的切换不需要涉及任何系统调用或任何阻塞调用，即协程只在一个线程中执行，是在子程序之间的切换，发生在用户态上，而线程的阻塞状态是由操作系统内核来完成，发生在内核态上，因此协程相比线程来说更加节省开销，主要是线程创建和切换需要资源较多。

协程一般适用于 IO 阻塞且需要大量并发的场景，当发生 IO 阻塞时，由协程的调度器进行调度，将数据流中断掉，并且记录当前的栈信息，阻塞完后立刻再通过线程恢复协程栈，并把阻塞的结果放到这个线程上去运行。

#### 进程、线程以及协程之间的关系

操作系统中进程、线程以及协程的大致关系如下图所示：

![2](./images/5-2.jpeg)

由上图可以看出同一进程中的多个线程是共享该进程中的系统资源，也就是说线程不能脱离于进程而单独存在。另外，进程之间是有相应的程序、数据块和地址空间的。一个线程中可以有多个子程序（协程），协程之间是由代码逻辑来控制切换的，即基于用户态上（由用户程序控制）。

## 多进程并发服务实现

一般服务器按实现方式可以分为单例类和并发类服务，我们平时用 C 语言编写的简单 Socket 服务端和客户端通信的例子是属于单例类的，即服务端每次只能将一个客户端的请求处理完后才能处理下一个请求，其特点是实现起来比较简单，系统资源消耗小，但是处理效率比较低。

而往往在实际应用中，并发类服务用的比较多，通常不会让一个服务长时间处理一个客户端请求，而是需要服务端可以同时处理多个客户请求的到来，这种可以同时处理多个客户端请求的服务，我们一般称为并发服务，并发服务的优势就是处理效率高，但实现起来有点复杂。

在 Linux 环境下通常有三种实现并发服务的方式：多进程并发服务器，多线程并发服务器和IO 多路复用。

先来看下多进程并发服务器的实现。实现多进程并发服务器时，我们需要考虑和观察的以下几个点：

1. 父进程创建套接字（文件描述符）的数量，注意：父进程中需要 close 关闭 accept 返回的新文件描述符
2. 系统内允许创建的进程个数，一般与内存大小有关，注意关注内存
3. 是否有进程创建过多的情况，创建过多会影响服务性能，注意观察资源消耗情况

多进程并发服务器实现流程图如下：

![4](./images/5-4.jpeg)

#### 基于 C 语言实现

以下是基于 C 语言实现多进程并发服务通信。

- 服务端 server

服务端实现一般有以下几个步骤：
1. 创建套接字 Socket
2. 绑定地址结构 Bind
3. 监听套接字（设置监听上限） Listen
4. 接收客户端连接请求 Accept
5. 创建子进程 fork
6. 与客户端通信并处理数据 Read、Write
7. 关闭套接字 Close

服务端完整代码如下：

```c
/*
    file: multiprocess-server.c
    build-cmd: gcc multiprocess-server.c socket-comm.c -o multiprocess-server
*/

#include <stdio.h>
#include <string.h>
#include <netinet/in.h>
#include <arpa/inet.h>
#include <signal.h>
#include <sys/wait.h>
#include <sys/types.h>
#include <unistd.h>
#include <ctype.h>

#include "socket-comm.h"

#define MAXLINE 60      // 定义缓冲区长度
#define SERV_PORT 8080  // 定义端口号

// 信号捕捉函数，完成子进程回收
void do_child_sign(int num)
{
    // 回收子进程用 waitpid，参数 0：同一个进程组，NULL：不关心状态，WNOHANG：不挂起（非阻塞）
	while (waitpid(0, NULL, WNOHANG) > 0);
}

// 主函数（入口函数）
int main(void)
{
	// 变量的声明
	struct sockaddr_in servaddr, cliaddr;
	socklen_t cliaddr_len;
	int listenfd, connfd;
	char buf[MAXLINE];
	char str[INET_ADDRSTRLEN];
	int i, n;
	pid_t pid;

    // 信号捕捉处理
	struct sigaction newact;
	newact.sa_handler = do_child_sign;
	sigemptyset(&newact.sa_mask);
	newact.sa_flags = 0;
	sigaction(SIGCHLD, &newact, NULL);

    // 1. 创建并监听套接字，返回 lfd 文件描述符
	listenfd = Socket(AF_INET, SOCK_STREAM, 0);

	bzero(&servaddr, sizeof(servaddr));
	servaddr.sin_family = AF_INET;
	servaddr.sin_addr.s_addr = htonl(INADDR_ANY);
	servaddr.sin_port = htons(SERV_PORT);

    // 2. 绑定地址结构
	Bind(listenfd, (struct sockaddr *)&servaddr, sizeof(servaddr));

    // 3. 设置监听的上限，此处为 10
	Listen(listenfd, 10);

	printf("Accepting connections ...\n");
	while (1) {
		cliaddr_len = sizeof(cliaddr);
        // 4. 接收客户端连接请求
		connfd = Accept(listenfd, (struct sockaddr *)&cliaddr, &cliaddr_len);

        // 5. fork 创建子进程，pid=0
		pid = fork();
		if (pid == 0) {
            // 创建子进程后，关闭用于建立连接的套接字 lfd
			Close(listenfd);
			// 6. 通信并进行数据处理
			while (1) {
                // 从客户端读取数据
				n = Read(connfd, buf, MAXLINE);
				if (n == 0) {
					printf("the other side has been closed.\n");
					break;
				}
                // 打印接收到客户端的地址信息，IP地址和端口号
				printf("received from %s at PORT %d\n",
						inet_ntop(AF_INET, &cliaddr.sin_addr, str, sizeof(str)),
						ntohs(cliaddr.sin_port));
				for (i = 0; i < n; i++)
					buf[i] = toupper(buf[i]);   // 小写转大写
				Write(connfd, buf, n);          // 写回给客户端
			}
			Close(connfd);
			return 0;
		} else if (pid > 0) {
			Close(connfd);      // 关闭用于与客户端通信的套接字 cfd
		} else
			perr_exit("fork");  // fork 创建子进程有误
	}
	Close(listenfd);            // 7. 关闭套接字
	return 0;
}
```

- 客户端 client

客户端实现一般有以下几个步骤：
1. 创建通信套接字 Socket
2. 建立连接 Connect
3. 与服务端通信，数据处理 Write、Read
4. 关闭套接字 Close

客户端完整代码如下：

```c
/*
    file: multiprocess-client.c
    build-cmd: gcc multiprocess-client.c socket-comm.c -o multiprocess-client
*/

#include <stdio.h>
#include <string.h>
#include <unistd.h>
#include <netinet/in.h>
#include <arpa/inet.h>

#include "socket-comm.h"

#define MAXLINE 60
#define SERV_PORT 8080  // 连接到服务端的端口号


// 客户端主函数（入口函数）
int main(int argc, char *argv[])
{
	// 变量声明
	struct sockaddr_in servaddr;
	char buf[MAXLINE];
	int sockfd, n;

    // 1. 创建 socket 套接字
	sockfd = Socket(AF_INET, SOCK_STREAM, 0);

    // 地址结构信息赋值
	bzero(&servaddr, sizeof(servaddr));
	servaddr.sin_family = AF_INET;
	inet_pton(AF_INET, "127.0.0.1", &servaddr.sin_addr);
	servaddr.sin_port = htons(SERV_PORT);

    // 2. 建立连接
	Connect(sockfd, (struct sockaddr *)&servaddr, sizeof(servaddr));

    // 3. 循环获取服务端发过来的数据
	while (fgets(buf, MAXLINE, stdin) != NULL) {
        // 发送数据到服务端
		Write(sockfd, buf, strlen(buf));
        // 读取服务端发过来的数据
		n = Read(sockfd, buf, MAXLINE);
		if (n == 0) {
			printf("the other side has been closed.\n");
			break;
		} else
            // 输出接收到的数据 
			Write(STDOUT_FILENO, buf, n);
	}
    // 4. 关闭文件套接字
	Close(sockfd);
	return 0;
}

```

## 多线程并发服务实现

在使用线程模型开发服务器时需考虑以下问题：

1. 调整进程内最大文件描述符上限
2. 线程如有共享数据，考虑线程同步
3. 服务于客户端线程退出时，退出处理。（退出值，分离态）
4. 系统负载，随着链接客户端增加，导致其它线程不能及时得到 CPU

#### 基于 C 语言实现

- 服务端 server

```C
/*
    file: multithread-server.c
*/

#include <stdio.h>

```

- 客户端 client

```c
/*
    file: duobo-client.c
*/

#include <netinet/in.h>
```

## 多协程并发服务

#### 基于 Go 语言实现

- 服务端 server

现在已经完成了客户端与服务端的通信，但是服务端只能接收一个用户发送过来的数据，怎样接收多个客户端发送过来的数据，实现一个高效的并发服务器呢？

Accept()函数的作用是等待客户端的链接，如果客户端没有链接，该方法会阻塞。如果有客户端链接，那么该方法返回一个 Socket 负责与客户端进行通信。所以，每来一个客户端，该方法就应该返回一个 Socket 与其通信，因此，可以使用一个死循环，将 Accept()调用过程包裹起来。

需要注意的是，实现并发处理多个客户端数据的服务器，就需要针对每一个客户端连接，单独产生一个 Socket，并创建一个单独的 goroutine 与之完成通信。

```go

```

将客户端的数据处理工作封装到 HandleConn 方法中，需将 Accept()返回的 Socket 传递给该方法，变量 conn 的类型为：net.Conn。可以使用 conn.RemoteAddr()来获取成功与服务器建立连接的客户端 IP 地址和端口号：

客户端可能持续不断的发送数据，因此接收数据的过程可以放在 for 循环中，服务端也持续不断的向客户端返回处理后的数据。

- 客户端 client

客户端不仅需要持续的向服务端发送数据，同时也要接收从服务端返回的数据。因此可将发送和接收放到不同的协程中。

主协程循环接收服务器回发的数据（该数据应已转换为大写），并打印至屏幕；子协程循环从键盘读取用户输入数据，写给服务器。读取键盘输入可使用 os.Stdin.Read(str)。定义切片 str，将读到的数据保存至 str 中。

这样，客户端也实现了多任务。

客户端代码:

```go

```

## 多路 I/O 转接服务实现

多路 IO 转接服务器也叫做多任务 IO 服务器。该类服务器实现的主旨思想是，不再由应用程序自己监视客户端连接，取而代之由内核替应用程序监视文件。
主要使用的方法有三种

#### select 模式

1. select 能监听的文件描述符个数受限于 FD_SETSIZE,一般为 1024，单纯改变进程打开的文件描述符个数并不能改变 select 监听文件个数
2. 解决 1024 以下客户端时使用 select 是很合适的，但如果链接客户端过多，select 采用的是轮询模型，会大大降低服务器响应效率，不应在 select 上投入更多精力

- 服务端 server

```c
/*
    file: domain-server.c
*/

#include <stdlib.h>
```

- 客户端 client

```c
/*
    file: domain-client.c
*/

#include <stdio.h>
```

#### epoll 模式

epoll 是 Linux 下多路复用 IO 接口 select/poll 的增强版本，它能显著提高程序在大量并发连接中只有少量活跃的情况下的系统 CPU 利用率，因为它会复用文件描述符集合来传递结果而不用迫使开发者每次等待事件之前都必须重新准备要被侦听的文件描述符集合，另一点原因就是获取事件的时候，它无须遍历整个被侦听的描述符集，只要遍历那些被内核 IO 事件异步唤醒而加入 Ready 队列的描述符集合就行了。
目前 epell 是 linux 大规模并发网络程序中的热门首选模型。
epoll 除了提供 select/poll 那种 IO 事件的电平触发（Level Triggered）外，还提供了边沿触发（Edge Triggered），这就使得用户空间程序有可能缓存 IO 状态，减少 epoll_wait/epoll_pwait 的调用，提高应用程序效率。
可以使用 cat 命令查看一个进程可以打开的 socket 描述符上限。
cat /proc/sys/fs/file-max
如有需要，可以通过修改配置文件的方式修改该上限值。
sudo vi /etc/security/limits.conf
在文件尾部写入以下配置,soft 软限制，hard 硬限制。如下图所示。
_ soft nofile 65536
_ hard nofile 100000

基础 API

1.  创建一个 epoll 句柄，参数 size 用来告诉内核监听的文件描述符的个数，跟内存大小有关。
    #include <sys/epoll.h>
    int epoll_create(int size) size：监听数目
2.  控制某个 epoll 监控的文件描述符上的事件：注册、修改、删除。
    #include <sys/epoll.h>
    int epoll_ctl(int epfd, int op, int fd, struct epoll_event \*event)
    epfd： 为 epoll_creat 的句柄
    op： 表示动作，用 3 个宏来表示：
    EPOLL_CTL_ADD (注册新的 fd 到 epfd)，
    EPOLL_CTL_MOD (修改已经注册的 fd 的监听事件)，
    EPOLL_CTL_DEL (从 epfd 删除一个 fd)；
    event： 告诉内核需要监听的事件

        struct epoll_event {
        	__uint32_t events; /* Epoll events */
        	epoll_data_t data; /* User data variable */
        };
        typedef union epoll_data {
        	void *ptr;
        	int fd;
        	uint32_t u32;
        	uint64_t u64;
        } epoll_data_t;

        EPOLLIN ：	表示对应的文件描述符可以读（包括对端SOCKET正常关闭）
        EPOLLOUT：	表示对应的文件描述符可以写
        EPOLLPRI：	表示对应的文件描述符有紧急的数据可读（这里应该表示有带外数据到来）
        EPOLLERR：	表示对应的文件描述符发生错误
        EPOLLHUP：	表示对应的文件描述符被挂断；
        EPOLLET： 	将EPOLL设为边缘触发(Edge Triggered)模式，这是相对于水平触发(Level Triggered)而言的
        EPOLLONESHOT：只监听一次事件，当监听完这次事件之后，如果还需要继续监听这个socket的话，需要再次把这个socket加入到EPOLL队列里

3.  等待所监控文件描述符上有事件的产生，类似于 select()调用。
    #include <sys/epoll.h>
    int epoll_wait(int epfd, struct epoll_event \*events, int maxevents, int timeout)
    events： 用来存内核得到事件的集合，
    maxevents： 告之内核这个 events 有多大，这个 maxevents 的值不能大于创建 epoll_create()时的 size，
    timeout： 是超时时间
    -1： 阻塞
    0： 立即返回，非阻塞 >0： 指定毫秒
    返回值： 成功返回有多少文件描述符就绪，时间到时返回 0，出错返回-1

代码实现如下：

- 服务端 server

```c

```

- 客户端 client

```c

```

## 实验总结

以上是 UDP Socket 编程的相关实现说明，通过学习完本实验，大家可以发现，大部分的代码实现是上一个实验有讲到过的，只有少部分的实现方式和用法有点不一样，注释里也有说明，换汤不换药，说明前面的实验学习对后面的实战实验有一定帮助的。本实验也通过几个不同的案例实现，来讲解 UDP 的实现原理和具体使用方法，在网络编程中跟 TCP 同样重要，所以也希望同学们可以跟着实验一步一步地在电脑上敲一遍案例代码，并修改相应的变量或参数来达到运行的效果，进一步的巩固。
